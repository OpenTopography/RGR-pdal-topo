{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Fri Nov 19 14:16:09 2021\n",
    "\n",
    "@author: sjohnstone\n",
    "\"\"\"\n",
    "\n",
    "#Import libraries for requesting downloads from websites, and for navigating operating system file structure\n",
    "import urllib\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pointCloudCreation as pcc #Some wrappers for PDAL pipeline creation and execution\n",
    "\n",
    "#Specify the path to where we want to save things\n",
    "savePath = 'D:\\\\ResearchProjects\\\\Blanca_lidar\\\\Blanca_LAS_test\\\\tiny_request' #Where do we want to store all the files we download?\n",
    "\n",
    "#Make the directory if it doesnt exist\n",
    "if not(os.path.isdir(savePath)):\n",
    "    os.mkdir(savePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8b6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When gridding up data we can specify extents and projections if we want to transform and crop the data\n",
    "\n",
    "#We could also use this information to obtain the list of files using the national maps APIs\n",
    "# https://apps.nationalmap.gov/help/documents/TNMAccessAPIDocumentation/TNMAccessAPIDocumentation.pdf \n",
    "\n",
    "#Lets grid this up\n",
    "extent = ([-105.578, -105.544],[37.630, 37.604]) #([MinX,MaxX],[MinY,MaxY])z\n",
    "extent_epsg = 4326 #SRS for extent\n",
    "out_epsg = 32613 #SRS for output - this is WGS84 UTM Zone 13 N\n",
    "out_extent = pcc.reproject_extent(extent,extent_epsg,out_epsg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c192c",
   "metadata": {},
   "source": [
    "**Batch downloading from the national map**\n",
    "\n",
    "Here we will look at two examples of batch downloading from The National Map; one with lidar data, a second with gridded data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e52f3",
   "metadata": {},
   "source": [
    "**Downloading LAS data with a Special condition**\n",
    "\n",
    "This particular region of interest has data sourced from three collections; one in 2011 and two in 2020. The 2011 data is quite out of date, and superceeded by the 2020 data (change detection anyone?). For just making a DEM, we are going to ignore the 2011 data (so don't want to download its pointcloud).  We are going to insert a special condition to *not* download files that are associated with this collection, by checking each download path in the file we got from TNM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1622b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_4962.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_4963.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_4964.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_5062.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_5063.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_5064.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_5162.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_5163.laz\n",
      "Working on: USGS_LPC_CO_SanLuisJuanMiguel_2020_D20_13S_DB_5164.laz\n"
     ]
    }
   ],
   "source": [
    "#BEWARE! This can download a huge amount of data!\n",
    "pathToTNMList = 'batch_download_tiny_example.txt'\n",
    "\n",
    "#Open the path file for reading\n",
    "with open(pathToTNMList,'r') as f:\n",
    "    #For each line (a path to a file in the TNM download list)\n",
    "    for line in f:\n",
    "        #Strip off any whitespace\n",
    "        line = line.strip()\n",
    "        #Get the 'name' of this file as the end of the filepath, we'll use this to save the file\n",
    "        name = line.split('/')[-1]\n",
    "        \n",
    "        #Here we will introduce our special condition, making sure we don't have anything that contains the \n",
    "        #name of the old collection. If you didn't want this you'd have to remove this conditional statement.\n",
    "        if not ('CO_San-Luis-Valley_2011' in line):\n",
    "            #Keep track of progress\n",
    "            print('Working on: {}'.format(name))\n",
    "            urllib.request.urlretrieve(line,os.path.join(savePath,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f933259",
   "metadata": {},
   "source": [
    "**Using PDAL to merge / grid the data**\n",
    "\n",
    "We can use PDAL to merge and grid the data. However, PDAL uses a moving circular window to aggregate the point cloud onto a DEM. In this example (where there is actually a fairly low density of ground returns in places) this does not seem to produce as clean of results as you mike get from creating a TIN using something like lastools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2debcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Lets define some things for creating a DEM from the point cloud #########\n",
    "in_epsg = 1 #For now this doesn't matter - it just needs to be different than out_epsg to reproject the point cloud\n",
    "grid_name = 'small_test_rad1p4_tileTest' #What will we save the DEM as?\n",
    "las_directory = savePath #Where are all the '.la (s/z) files we want to work with?\n",
    "cell_size = 1.0 #What pixel size do we want\n",
    "radius = cell_size*np.sqrt(2) #The radius of the window used by PDALs gridding to aggregate the point cloud onto a grid\n",
    "gridding_method = 'idw' #'idw', 'mean', 'min', 'max', 'count', 'std' see: https://pdal.io/stages/writers.gdal.html\n",
    "doReclassify = False #Do we want to reclassify the point cloud?\n",
    "\n",
    "#Build the 'pipeline' for PDAL - this is the series of processing steps\n",
    "pipeline = pcc.gather_las_files_build_dem_creation_pipeline(in_epsg,out_epsg,grid_name,las_directory,cell_size = cell_size,\n",
    "                           radius = radius,nodatavalue = -9999, doReclassify = doReclassify,doSavePointCloud = False,\n",
    "                                        pointResolution = None,gridding_method = gridding_method)\n",
    "\n",
    "pcc.run_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0299dee",
   "metadata": {},
   "source": [
    "**PDAL also allows you to tile the datasets you are creating**\n",
    "\n",
    "This can be helpful if you want to limit the size of the DEMs you are creating, however the way I have things implemented herepdal will still load all the point cloud data files in the specified path into memory and uncompress them (so they will take up a lot more ram then disk space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc400b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tiling test\n",
    "grid_name = 'tile_test'\n",
    "tile_length = 2000.0 #Dimension of each square tile\n",
    "buffer = 10 # Amount of overlap between each tile\n",
    "cell_size = 2.0 #What do we want the pixel size to be\n",
    "radius = cell_size*np.sqrt(2) #What area to we want to look in when gridding?\n",
    "gridding_method = 'idw' #'idw', 'mean', 'min', 'max', 'count', 'std' see: https://pdal.io/stages/writers.gdal.html\n",
    "\n",
    "doReclassify = False\n",
    "doSavePointCloud = False\n",
    "\n",
    "#Build the 'pipeline' for PDAL - this is the series of processing steps\n",
    "pipeline = pcc.gather_las_files_build_dem_tile_creation_pipeline(in_epsg,out_epsg, grid_name, las_directory, \n",
    "                                                                 tile_length = tile_length, buffer = buffer,\n",
    "                                                                 cell_size = cell_size, radius = radius,\n",
    "                                                                 nodatavalue = -9999,\n",
    "                                                                doReclassify = doReclassify,\n",
    "                                                                 doSavePointCloud = doSavePointCloud)\n",
    "\n",
    "#Run the pipeline\n",
    "pcc.run_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad46f53",
   "metadata": {},
   "source": [
    "**We can download DEM data in the same way we did above**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21177915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: USGS_1M_13_x44y417_CO_SanLuisJuanMiguel_2020_D20.tif\n",
      "Working on: USGS_1M_13_x45y417_CO_SanLuisJuanMiguel_2020_D20.tif\n"
     ]
    }
   ],
   "source": [
    "#BEWARE! This can download a huge amount of data!\n",
    "pathToTNMList = 'batch_download_tiny_example_dem.txt'\n",
    "\n",
    "#Open the path file for reading\n",
    "with open(pathToTNMList,'r') as f:\n",
    "    #For each line (a path to a file in the TNM download list)\n",
    "    for line in f:\n",
    "        #Strip off any whitespace\n",
    "        line = line.strip()\n",
    "        #Get the 'name' of this file as the end of the filepath, we'll use this to save the file\n",
    "        name = line.split('/')[-1]\n",
    "        \n",
    "        #Keep track of progress\n",
    "        print('Working on: {}'.format(name))\n",
    "        urllib.request.urlretrieve(line,os.path.join(savePath,name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b167a4",
   "metadata": {},
   "source": [
    "**We can use GDAL to merge, crop, and reproject these DEMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9206b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to search a directory other than the one this notebook is in, prepend a path (e.g., os.path.join(path//to//folder,'*.tif'))\n",
    "pathToTifs = os.path.join(savePath,'*.tif') #Get the names of the files of interest (could alternatively save this above)\n",
    "outputName = 'mergedDEMs.tif' #How do we want to name the mer\n",
    "pixel_size = 1.0\n",
    "files = glob(pathToTifs)\n",
    "\n",
    "#This is a wrapper on some GDAL functions\n",
    "pcc.merge_warp_dems(files,outputName,out_extent,out_epsg,pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729742a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
